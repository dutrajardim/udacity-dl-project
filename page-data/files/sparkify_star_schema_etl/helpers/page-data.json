{
    "componentChunkName": "component---src-templates-python-file-tsx",
    "path": "/files/sparkify_star_schema_etl/helpers",
    "result": {"pageContext":{"file":{"relativePath":"sparkify_star_schema_etl/helpers.py","name":"helpers","fields":null,"functions":[{"name":"s3_path_exists","line":3,"docstring":{"arguments":"sc: The spark context to be used\n        path: The path of the bucket.\n    Return:\n        It returns True if the bucket exists and\n        flase otherwise.","description":"This function is responsible for cheching if a s3 path\n        exists using a spark context.\n        Thanks @Dror, @Kini and @rosefun (https://stackoverflow.com/questions/55589969/how-to-check-a-file-folder-is-present-using-pyspark-without-getting-exception)","returns":""}},{"name":"_compose","line":25,"docstring":{"arguments":"functions: all functions in the order that the chain will be coposed.\n        All functions accept a tuple with a spark data frame and a schema, and\n        returns the tuple with the transformations applied","description":"This function is responsible for compose a new function \n        chaining a sequence of functions. The output of the\n        previous function is given to the next one as input.","returns":"A composed function that accept one argument to be precessed\n        by the chain. The returned function accept a tuple where\n        the first element is a spark data frame and the second is\n        a spark schema."}},{"name":"create_basic_pipeline","line":53,"docstring":{"arguments":"rename_transformations: A dictionary where the keys are \n        the desired column names and values are the current names of the \n        column in the spark data frame.\n        cast_tranformations: A dictionary where the keys are the current\n        column names and the values are spark sql expressions to be applied.","description":"This function is responsible for compose a function that\n        will apply rename and cast transformations to a given spark\n        data frame","returns":"It returns a spark data frame with the transformations applied."}},{"name":"_rename_columns","line":71,"docstring":{"arguments":"input_table: a tuple where the first element is a spark data frame\n            and the second is a spark schema.","description":"This function is responsible for apply the rename transformations\n            given as arguments to create_basic_pipeline.","returns":"The given function argumens with the lazy transformations applied to the\n            spark data frame."}},{"name":"_select_expr_columns","line":107,"docstring":{"arguments":"input_table: a tuple where the first element is a spark data frame\n            and the second is a spark schema.","description":"This function is responsible for apply cast transformations and select\n            the columns listed in the schema in the given spark data frame.","returns":"The given function argumens with the lazy transformations applied to the\n            spark data frame."}}]},"site":{"siteMetadata":{"githubProjectUrl":"https://github.com/dutrajardim/udacity-dl-project/blob/main"}}}},
    "staticQueryHashes": ["29811882"]}