---
title: "home"
---

Hi! My name is Rafael Dutra...

## Project introduction
This project is an exercise I have done for the Data Engineering Course at Udacity. The core idea of the activity is to build an ETL pipeline using the Pyspark framework to extract data from an S3 bucket, create tables based on the star schema, and save those tables in another S3 bucket.\
Two datasets were provided, song dataset and log dataset.\
The song dataset is a subset of data from the [Million Song Dataset](https://labrosa.ee.columbia.edu/millionsong/). This dataset contains metadata about songs and the artists of those songs.\
The log dataset consists of log files of users' activities from imaginary music streaming app, generated by an [event simulator](https://github.com/Interana/eventsim) based on the Million Song Dataset.

### What you will find here
For this project I created the proposed ETL scripts using python and have been running tests in a local kubernates environment and in a AWS Elastic MapReduce (EMR) cluster.\
For the k8s enviroment, I've installed the [Minio Operator](https://github.com/minio/operator) to place the simple storage service (S3) role and the [Spark on K8s Operator](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator) to manage Spark applications.\
In this way, you will find in the project's Github repository the follow base structure.

- **sparkify_star_schema_etl** (folder): description..

And, in the documentation page the follow navigation links:

- **Docstrings** : description...

### Installation
Download the project source code and create a virtual python environment (using python >= 3.5) with the follow commands in the linux shell console. 


```console
$ git clone https://github.com/dutrajardim/udacity-dl-project.git
$ ...
$ cd udacity-dl-project
$ python -m venv .venv
$ ...
$ source .venv/bin/activate
```

> ℹ️ **_NOTE:_** If you want to use the notebooks with spark in client mode on kubernetes, as setted in sparkconf.cfg.exemple file (in the project root directory), make sure to use python of the same version as setted in the docker executors image. Differents versions of python in driver and executors leads to errors.

To compile python dependencies as a package file that can be send to spark cluster you can run the command below. It's going to generate a egg file in the dist folder. This file we can set as argument to pyspark --py-file to be used as dependence for the main script sparkify_script.py.

```console
$ python setup.py bdist_egg
```

To install install it in the local environment execute as follow. It will allow us to import the modules in notebooks files. 

```console
$ python setup.py install
```

> ℹ️ **_NOTE:_** For more information about python package setup you can visite the [SetupTools Documentation here.](https://setuptools.pypa.io/en/latest/)

The modules egg file and the main script sparkify_script.py can be uploaded to S3 to make it remotely availeble for spark driver and workers download it.

Exemples of usage in AWS EMR can be found in the shell_scripts folder. There is also in the k8s folder examples of usage with Spark on K8s Operator and Minio.

> ℹ️ **_NOTE:_** For Spark on K8s Operator environment I first create a minio share link to set --py-files, makint the egg file avileble over http protocol.

To run sparkify_script.py an argument informing which module to execute is expected. So, to run sparkify_star_schema_etl the follow command need to be performed by the spark submit:

```console
$ sparkify_script.py <olap_job or start_schema_job>
```

## Documentation
[Gatsby.js](https://www.gatsbyjs.com).\

### Building
