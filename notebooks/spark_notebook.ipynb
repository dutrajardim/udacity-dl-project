{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import round as spark_round\n",
    "\n",
    "sys.path.append('..')\n",
    "from etl import create_spark_session\n",
    "from schemas import song_data_schema, log_data_schema, user_table_schema, song_table_schema, artist_table_schema, songplay_table_schema, time_table_schema\n",
    "from basic_pipeline import create_basic_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.38:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://127.0.0.1:16443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Udacity - Data Lake Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2449bc8730>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = create_spark_session(local=True)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_data = spark.read.format('json').schema(log_data_schema).option(\"recursiveFileLookup\",True).load('s3a://udacity-dend/log_data')\n",
    "df_song_data = spark.read.format('json').schema(song_data_schema).option(\"recursiveFileLookup\",True).load('s3a://udacity-dend/song_data')\n",
    "\n",
    "df_joined = df_log_data.join(\n",
    "    df_song_data, \n",
    "    on=(df_log_data.song == df_song_data.title) & (spark_round(df_log_data.length, 4) == spark_round(df_song_data.duration, 4)),\n",
    "    how='LEFT'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict - get column name from expression\n",
    "rename_transformations = {\n",
    "    'start_time': 'ts',\n",
    "    'user_id': 'userId',\n",
    "    'first_name': 'firstName',\n",
    "    'last_name': 'lastName',\n",
    "    'name': 'artist_name',\n",
    "    'location': 'artist_location',\n",
    "    'latitude': 'artist_latitude',\n",
    "    'longitude': 'artist_longitude',\n",
    "    'session_id': 'sessionId',\n",
    "    'user_agent': 'userAgent' \n",
    "}\n",
    "\n",
    "cast_transformations = {\n",
    "    'start_time': 'to_timestamp(start_time / 1000) as start_time', \n",
    "    'user_id': 'INT(user_id) as user_id',\n",
    "}\n",
    "\n",
    "time_transformations = {\n",
    "  \"hour\": \"hour(start_time) as hour\",\n",
    "  \"day\": \"dayofmonth(start_time) as day\",\n",
    "  \"week\": \"weekofyear(start_time) as week\",\n",
    "  \"month\": \"month(start_time) as month\",\n",
    "  \"year\": \"year(start_time) as year\",\n",
    "  \"weekday\": \"dayofweek(start_time) as weekday\",\n",
    "}\n",
    "\n",
    "basic_pipeline = create_basic_pipeline(rename_transformations=rename_transformations, cast_transformations=cast_transformations)\n",
    "time_basic_pipeline = create_basic_pipeline(cast_transformations=time_transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = basic_pipeline((df_log_data, user_table_schema)).distinct()\n",
    "df_songs = basic_pipeline((df_song_data, song_table_schema))\n",
    "df_artists = basic_pipeline((df_song_data, artist_table_schema)).distinct()\n",
    "df_songplays = basic_pipeline((df_joined, songplay_table_schema))\n",
    "df_times = time_basic_pipeline((df_songplays, time_table_schema)).distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\",\"dynamic\")\n",
    "df_users.write \\\n",
    "    .partitionBy('user_id') \\\n",
    "    .option('schema', user_table_schema) \\\n",
    "    .format('parquet') \\\n",
    "    .mode('overwrite') \\\n",
    "    .save('s3a://dutrajardim/udacity-dl-project/users.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df_songs.write \\\n",
    "    .partitionBy(['year', 'artist_id']) \\\n",
    "    .option('schema', song_table_schema) \\\n",
    "    .format('parquet') \\\n",
    "    .mode('overwrite') \\\n",
    "    .save('s3a://dutrajardim/udacity-dl-project/songs.parquet')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df_artists.write \\\n",
    "    .partitionBy(['artist_id']) \\\n",
    "    .option('schema', artist_table_schema) \\\n",
    "    .format('parquet') \\\n",
    "    .mode('overwrite') \\\n",
    "    .save('s3a://dutrajardim/udacity-dl-project/artists.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month\n",
    "df_songplays \\\n",
    "    .withColumn('year', year('start_time')) \\\n",
    "    .withColumn('month', month('start_time')) \\\n",
    "    .write \\\n",
    "    .partitionBy(['year', 'month'])\n",
    "    .option('schema', songplay_table_schema) \\\n",
    "    .mode('overwrite') \\\n",
    "    .save('s3a://dutrajardim/udacity-dl-project/songplays.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f00076993a79c6ae49a7e3129977fe8e9ce55828899303a503ea49aa3f668a3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
